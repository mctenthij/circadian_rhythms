{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load all required python modules for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a random sample of users using OSoMe\n",
    "\n",
    "After obtaining the circadian rhythms of the \\`\\`Depressed'' cohort, we want to compare these rhythms to a general selection of Twitter users. This notebook describes the process of the formation of that random sample of Twitter users, which we refer to as the \\`\\`Random'' cohort.\n",
    "\n",
    "## Sample design\n",
    "\n",
    "Social Media platforms typically evolve over time. This is also reflected in the individuals that use the platform. Therefore, we build our \\`\\`Random'' cohort of Twitter users by using characteristics of the \\`\\`Depressed'' cohort as follows.\n",
    "\n",
    "1. We determine the per month distribution of the profile creation dates of the \\`\\`Depressed'' cohort. \n",
    "2. We sample users at random from OSoMe to create a user pool that could be used in our \\`\\`Random'' cohort.\n",
    "3. We randomly choose Twitter users from the OSoMe user pool in such a way that our \\`\\`Random'' cohort has the same distribution of profile creation dates as the \\`\\`Depressed'' cohort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Depression users distribution\n",
    "\n",
    "First, we construct a list that contains all user IDs of users in the \\`\\`Depressed'' cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total we have 1211 Twitter users in the ``Depressed'' cohort, out of which 691 have time zone information.\n"
     ]
    }
   ],
   "source": [
    "label_fn = \"data/diagnosis_labelling_depression_final.tsv\"\n",
    "labels = pd.read_csv(label_fn, sep=\"\\t\", index_col=\"tweet_id\")\n",
    "\n",
    "dep_users = labels[labels.diag_final == \"1\"].user_id.unique()\n",
    "\n",
    "with open(\"data/depression_user_to_tz.json\") as jf:\n",
    "    tz_dict = json.load(jf)\n",
    "\n",
    "tz_dep_users = np.intersect1d(dep_users, list(tz_dict.keys()))\n",
    "\n",
    "        \n",
    "print(\"In total we have\", dep_users.size, \"Twitter users in the ``Depressed'' cohort, out of which\",\n",
    "      tz_dep_users.size, \"have time zone information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we obtain the information regarding the creation dates of these users using the Twitter API. The error messages that occurred for the users for which we could not obtain the profile creation date are displayed below.\n",
    "\n",
    "| Error Code | Error Message            | Number of occurrences |\n",
    "|------------|--------------------------|-----------------------|\n",
    "| 50         | User not found.          | 43                    |\n",
    "| 63         | User has been suspended. | 13                    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_sample_distribution = pd.read_csv(\"data/to_sample_distribution.tsv\", sep=\"\\t\", header=None, squeeze=True, index_col=0)\n",
    "to_sample_distribution.index.rename(\"per_month\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating a pool of users with OSoMe\n",
    "\n",
    "We obtained several weeks of tweet data from OSoMe, using the random sample option. We obtained three weeks of tweet data:\n",
    "   1. September 1st 2017 00:00 UTC to September 8th 2017 00:00 UTC\n",
    "   2. March 1st 2018 00:00 UTC to March 8th 2018 00:00 UTC\n",
    "   3. September 1st 2018 00:00 UTC to September 8th 2018 00:00 UTC\n",
    "   \n",
    "Based on the obtained results, we build a dataframe that lists the creation dates of the Twitter users in our sample. We exclude all users that are already in the \\`\\`Depressed'' cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, we obtain 588356 users, out of which 387509 provided a location and are not in our ``Depressed'' cohort.\n"
     ]
    }
   ],
   "source": [
    "filenames = [\"2017_09\", \"2018_03\", \"2018_09\"]\n",
    "users = {}\n",
    "locations = {}\n",
    "all_users = set()\n",
    "users_with_loc_not_D = set()\n",
    "\n",
    "for fn in filenames:\n",
    "    with gzip.open(\"data/do_not_share/user_sample_\"+fn+\".gz\") as doc:\n",
    "        for l in doc.readlines():\n",
    "            data = json.loads(l.decode(\"utf-8\").strip(\"\\n\"))\n",
    "            all_users.add(data[\"user\"][\"id_str\"])\n",
    "            if int(data[\"user\"][\"id_str\"]) not in dep_users:\n",
    "                users[data[\"user\"][\"id_str\"]] = datetime.strptime(data[\"user\"][\"created_at\"], \"%a %b %d %H:%M:%S %z %Y\")\n",
    "                if data[\"user\"].get(\"location\"):\n",
    "                    users_with_loc_not_D.add(data[\"user\"][\"id_str\"])\n",
    "                    locations[data[\"user\"][\"id_str\"]] = data[\"user\"][\"location\"]\n",
    "                    \n",
    "print(\"In total, we obtain\", len(all_users), \"users, out of which\", len(users_with_loc_not_D), \"provided a location and are not in our ``Depressed'' cohort.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dist = pd.DataFrame(index=users.keys())\n",
    "sample_dist[\"created_at\"] = pd.Series(data=users)\n",
    "sample_dist[\"locations\"] = pd.Series(data=locations)\n",
    "sample_dist[\"per_month\"] = sample_dist[\"created_at\"].apply(lambda x: x.strftime(\"%Y_%m\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want to sample users for which we can obtain the time zone information based on the dictionary we built for the depression time lines. Therefore, we load that dictionary and determine if we can obtain the time zone for each user in the sample. All users that have time zone information are stored in the list `with_tz_info`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, we obtain timezone information for 71277 users.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/tz_user_loc.json\", encoding=\"ISO 8859-1\") as doc:\n",
    "    loc_to_tz = json.loads(doc.read())\n",
    "\n",
    "def get_tz(x):\n",
    "    if loc_to_tz.get(x):\n",
    "        return loc_to_tz[x]\n",
    "    \n",
    "sample_dist[\"tz_info\"] = sample_dist[\"locations\"].apply(get_tz)\n",
    "with_tz_info = sample_dist[\"tz_info\"].dropna().index.values\n",
    "\n",
    "print(\"In total, we obtain timezone information for\", with_tz_info.size, \"users.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the profile creation date distribution for all users for which we can obtain a timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counts_per_month = sample_dist.loc[with_tz_info, :].groupby([\"per_month\"]).count()\n",
    "amounts_in_sample = sample_counts_per_month.loc[to_sample_distribution.index, \"created_at\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on `to_sample_distribution`, we find the maximum number of users that we can extract from the seed set in such a way that these users have the same distribution in creation month for all users in both the \\`\\`Depressed'' cohort. The values that have to be sampled from the `sample_dist` are stored in `to_sample_amounts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We sample 9525 Twitter users as our ``Random'' cohort, out of a total of 70155 Twitter users.\n"
     ]
    }
   ],
   "source": [
    "max_multiple = amounts_in_sample.sum() / to_sample_distribution.sum()\n",
    "N = np.where([np.all(amounts_in_sample >= x * to_sample_distribution) for x in np.arange(max_multiple)])[0].max()\n",
    "\n",
    "to_sample_amounts = N * to_sample_distribution\n",
    "print(\"We sample\", to_sample_amounts.sum(), \"Twitter users as our ``Random'' cohort, out of a total of\", amounts_in_sample.sum(), \"Twitter users.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these numbers, we can now sample the `sample_dist` per month with the number of user ids that we want to obtain from that month based on `to_sample_amounts`. The result is a list of user ids `random_user_sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_sample_dist = sample_dist.loc[with_tz_info, :].groupby([\"per_month\"])\n",
    "\n",
    "random_user_sample = []\n",
    "\n",
    "for (month, amount) in to_sample_amounts.iteritems():\n",
    "    uids = np.random.choice(grouped_sample_dist.groups[month], size=amount, replace=False)\n",
    "    random_user_sample = random_user_sample + uids.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add the timezone information for these users to a dictionary `sample_tz_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tz_dict = {}\n",
    "for u in random_user_sample:\n",
    "    sample_tz_dict[u] = sample_dist.loc[u, \"tz_info\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write this sample of users with their timezones to a file called `random_sample_user_to_tz.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```with open(\"data/random_sample_user_to_tz.json\", \"w\" as out):\n",
    "    json.dump(sample_tz_dict, out)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
